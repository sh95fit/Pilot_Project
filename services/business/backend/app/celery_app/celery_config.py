"""
Celery ì„¤ì • ë° MySQL ì—°ê²° ê´€ë¦¬
"""
from celery import Celery
from celery.schedules import crontab
from kombu import Queue, Exchange
import logging
from datetime import timedelta

from backend.app.core.config import settings

# ì›Œì»¤ ë¼ì´í”„ì‚¬ì´í´ ì‹œê·¸ë„
from celery.signals import (
    worker_process_init, # Worker í”„ë¡œì„¸ìŠ¤ ìµœì´ˆ ì‹œì‘ ì‹œ 1íšŒë§Œ ìˆ˜í–‰ (Mysql ì´ˆê¸°í™”, SSH Tunnel ìƒì„±)
    worker_process_shutdown, # Worker ì¢…ë£Œ ì‹œ 1íšŒë§Œ ìˆ˜í–‰ (ëª¨ë“  ì—°ê²° ì •ë¦¬, SSH Tunnel ë‹«ê¸°)
    task_prerun, # Task ì‹¤í–‰ ì§ì „ë§ˆë‹¤ ìˆ˜í–‰ (ì—°ê²° ìƒíƒœ í™•ì¸, í•„ìš”ì‹œ ì¬ì—°ê²°)
    task_postrun, # Task ì„±ê³µ ì‹œ ìˆ˜í–‰ (Pool ì‚¬ìš©ë¥  ì²´í¬)
    task_failure # Task ì‹¤íŒ¨ ì‹œ ìˆ˜í–‰ (ì—ëŸ¬ ì›ì¸ ë¶„ì„)
)
import asyncio
from backend.app.core.database.mysql_client import mysql_client


logger = logging.getLogger(__name__)

# =============================================================================
# Celery ì•± ìƒì„±
# =============================================================================

celery_app = Celery(
    "business_tasks",
    broker=settings.celery_broker_url,
    backend=None,
    include=[
        "app.celery_app.tasks.cohort_tasks",
    ]
)

# =============================================================================
# Celery ì„¤ì •
# =============================================================================

celery_app.conf.update(
    # íƒ€ì„ì¡´ ì„¤ì •
    timezone="Asia/Seoul",
    enable_utc=True,
    
    # íƒœìŠ¤í¬ ì„¤ì •
    task_serializer="json",
    accept_content=["json"],
    result_serializer="json",
    result_expires=3600,

    # Worker ì„¤ì •
    worker_prefetch_multiplier=4,
    worker_max_tasks_per_child=1000,  # ë©”ëª¨ë¦¬ ëˆ„ìˆ˜ ë°©ì§€
    task_acks_late=True,
    task_reject_on_worker_lost=True,
    
    # ì¬ì‹œë„ ì„¤ì •
    task_default_retry_delay=60,
    task_max_retries=3,
    
    
    # Beat ìŠ¤ì¼€ì¤„ëŸ¬ ì„¤ì •
    beat_schedule={
        # ë¯¸ì£¼ë¬¸ ê³ ê°ì‚¬ ë°ì´í„° ì—…ë°ì´íŠ¸
        "update-not-ordered-cohort": {
            "task": "cohort_tasks.update_not_ordered_cohort",
            "schedule": crontab(hour=14, minute=35, day_of_week="1-5"),
            "options": {"queue": "cohort"}
        },
        
        # ë¯¸ì£¼ë¬¸ ê³ ê°ì‚¬ ë°ì´í„° ì—…ë°ì´íŠ¸
        "update-pending-not-ordered-cohort": {
            "task": "cohort_tasks.update_pending_not_ordered_cohort",
            'schedule': crontab(minute='0,20,40', hour='9-14', day_of_week="1-5"),
            "options": {"queue": "cohort"}
        },
    
        # ì„œë¹„ìŠ¤ ì´ìš© ì¢…ë£Œ ê³ ê°ì‚¬ ë°ì´í„° ì—…ë°ì´íŠ¸
        "update-end-of-use-cohort": {
            "task": "cohort_tasks.update_end_of_use_cohort",
            'schedule': crontab(minute='1,21,41', hour='9-20', day_of_week="1-5"),
            "options": {"queue": "cohort"}
        },
        
        # í™œì„± ê³ ê° ë°ì´í„° ì—…ë°ì´íŠ¸
        "update-active-customer-cohort": {
            "task": "cohort_tasks.update_active_accounts_cohort",
            'schedule': crontab(minute='2,22,44', hour='9-20', day_of_week="1-5"),
            "options": {"queue": "cohort"}
        },
        
        # ì–´ë“œë¯¼ ìœ ì… ê³ ê° ë°ì´í„° ì—…ë°ì´íŠ¸
        "update-incoming-leads-cohort": {
            "task": "cohort_tasks.update_incoming_leads_cohort",
            "schedule": crontab(minute='3,23,43', hour="9-20", day_of_week="1-5"),
            "options": {"queue": "cohort"}
        },
        
        # í˜„ì¬ í™œì„± ê³ ê° ë°ì´í„° ì—…ë°ì´íŠ¸
        "update-now-active-accounts-cohort": {
            "task": "cohort_tasks.update_now_active_accounts_cohort",
            'schedule': crontab(minute='4,24,44', hour='9-20', day_of_week="1-5"),
            "options": {"queue": "cohort"}
        },
        
        # ìƒí’ˆë³„ ì£¼ë¬¸ ìˆ˜ëŸ‰ ë°ì´í„° ì—…ë°ì´íŠ¸        
        "update-product-sales-summary": {
            "task": "cohort_tasks.update_product_sales_summary_cohort",
            "schedule": crontab(minute='5,20,35,50'),
            "options": {"queue": "cohort"}
        },
        
        # ê³ ê°ì‚¬ ë‹¨ìœ„ ìƒí’ˆë³„ ì£¼ë¬¸ ìˆ˜ëŸ‰ ë°ì´í„° ì—…ë°ì´íŠ¸        
        "update-account-orders-summary": {
            "task": "cohort_tasks.update_account_orders_summary_cohort",
            "schedule": crontab(minute='6, 26, 46'),
            "options": {"queue": "cohort"}
        },
        
        # ì²´í—˜ ê³ ê°ì‚¬ ë°ì´í„° ì—…ë°ì´íŠ¸
        "update-trial-accounts-cohort": {
            "task": "cohort_tasks.update_trial_accounts_cohort",
            "schedule": crontab(minute='7, 27, 47'),
            "options": {"queue": "cohort"}
        },
        
        # ê³ ê°ìœ ì…ë¦¬ë“œ ë°ì´í„° ì—…ë°ì´íŠ¸
        "update-lead-applications-cohort": {
            "task": "cohort_tasks.update_lead_applications_cohort",
            "schedule": crontab(minute='*/10'),
            "options": {"queue": "cohort"}
        },
        
        # í™œì„± ê³ ê°ì‚¬ íˆìŠ¤í† ë¦¬ ë°ì´í„° ì—…ë°ì´íŠ¸
        "update-active-accounts-history-cohort": {
            "task": "cohort_tasks.update_active_accounts_history_cohort",
            "schedule": crontab(minute='*/10'),
            "options": {"queue": "cohort"}
        },
        
        
        # MySQL ì—°ê²° ëª¨ë‹ˆí„°ë§ (30ë¶„ë§ˆë‹¤)
        "monitor-mysql-health": {
            "task": "cohort_tasks.monitor_mysql_health",
            "schedule": timedelta(minutes=30),
            "options": {"queue": "cohort"}
        },
    },
    
    # í ì„¤ì •
    task_routes={
        "app.celery_app.tasks.cohort_tasks.*": {"queue": "cohort"},
    },
    
    task_queues=(
        Queue("cohort", Exchange("cohort"), routing_key="cohort"),
        Queue("default", Exchange("default"), routing_key="default"),
    ),
    
    # ì—°ê²° í’€ ì„¤ì •
    broker_connection_retry_on_startup=True,
    broker_connection_retry=True,
    broker_pool_limit=10,
)

logger.info("âœ… Celery app configured successfully")    



# =============================================================================
# MySQL ì—°ê²° ë¼ì´í”„ì‚¬ì´í´ ê´€ë¦¬
# =============================================================================

@worker_process_init.connect
def init_mysql_on_worker_start(**kwargs):
    """
    Worker í”„ë¡œì„¸ìŠ¤ ì‹œì‘ ì‹œ MySQL í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
    - SSH Tunnel ì„¤ì •
    - Event Loop ìƒì„±
    - Connection Poolì€ ì²« ìš”ì²­ ì‹œ Lazy ìƒì„±
    """
    try:
        # ìƒˆë¡œìš´ Event Loop ìƒì„±
        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)
        
        # MySQL í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” (SSH Tunnel)
        loop.run_until_complete(mysql_client.initialize())
        
        logger.info("âœ… Worker started: MySQL client initialized (SSH tunnel active)")
        
    except Exception as e:
        logger.error(f"âŒ Failed to initialize MySQL client on worker start: {e}", exc_info=True)
        raise


@task_prerun.connect
def ensure_mysql_connection_before_task(task_id, task, *args, **kwargs):
    """
    Task ì‹¤í–‰ ì§ì „ MySQL ì—°ê²° ìƒíƒœ í™•ì¸
    - Health check ìˆ˜í–‰
    - ë¬¸ì œ ë°œê²¬ ì‹œ ìë™ ì¬ì—°ê²°
    """
    # ëª¨ë‹ˆí„°ë§ TaskëŠ” ìŠ¤í‚µ (ë¬´í•œ ë£¨í”„ ë°©ì§€)
    if task.name == "cohort_tasks.monitor_mysql_health":
        return
    
    try:
        loop = asyncio.get_event_loop()
        
        # Health check
        is_healthy = loop.run_until_complete(mysql_client.health_check())
        
        if not is_healthy:
            logger.warning(
                f"âš ï¸ MySQL unhealthy before task {task.name} (id: {task_id}), "
                f"attempting reconnection..."
            )
            loop.run_until_complete(mysql_client._ensure_connection())
            logger.info("âœ… MySQL reconnected successfully")
        else:
            logger.debug(f"âœ… MySQL healthy, starting task {task.name}")
            
    except Exception as e:
        logger.error(
            f"âŒ Error checking MySQL before task {task.name}: {e}",
            exc_info=True
        )
        # Task ì‹¤í–‰ì€ ê³„ì† ì§„í–‰ (ì‹¤ì œ ì¿¼ë¦¬ ì‹œ ì¬ì‹œë„í•  ê²ƒ)


@task_postrun.connect
def monitor_pool_usage_after_task(task_id, task, retval, *args, **kwargs):
    """
    Task ì™„ë£Œ í›„ Connection Pool ì‚¬ìš©ë¥  ì²´í¬
    - 80% ì´ìƒ ì‚¬ìš© ì‹œ ê²½ê³  ë¡œê·¸
    """
    try:
        if not mysql_client.pools:
            return
        
        for db_name, pool in mysql_client.pools.items():
            in_use = pool.size - pool.freesize
            usage_percent = (in_use / pool.maxsize) * 100 if pool.maxsize > 0 else 0
            
            if usage_percent >= 80:
                logger.warning(
                    f"âš ï¸ High pool usage after task {task.name}: "
                    f"[{db_name}] {in_use}/{pool.maxsize} ({usage_percent:.1f}%)"
                )
            else:
                logger.debug(
                    f"Pool [{db_name}] usage: {in_use}/{pool.maxsize} ({usage_percent:.1f}%)"
                )
                
    except Exception as e:
        logger.error(f"Error monitoring pool usage: {e}")


@task_failure.connect
def handle_task_failure_with_connection_check(task_id, exception, *args, **kwargs):
    """
    Task ì‹¤íŒ¨ ì‹œ MySQL ì—°ê²° ì—ëŸ¬ì¸ì§€ í™•ì¸
    - ì—°ê²° ì—ëŸ¬ë©´ ë‹¤ìŒ Taskì—ì„œ ìë™ ì¬ì—°ê²°ë  ê²ƒì„ì„ ë¡œê·¸
    """
    try:
        error_msg = str(exception).lower()
        
        connection_error_keywords = [
            "lost connection", "can't connect", "connection refused",
            "broken pipe", "connection reset", "timeout", 
            "ssh tunnel", "no connection", "connection closed"
        ]
        
        is_connection_error = any(keyword in error_msg for keyword in connection_error_keywords)
        
        if is_connection_error:
            logger.error(
                f"âŒ Task failed due to MySQL CONNECTION error: {exception}",
                exc_info=True
            )
            logger.warning("ğŸ”„ Next task will attempt to reconnect MySQL automatically")
        else:
            logger.error(
                f"âŒ Task failed (non-connection error): {exception}",
                exc_info=True
            )
            
    except Exception as e:
        logger.error(f"Error in failure handler: {e}")


@worker_process_shutdown.connect
def cleanup_mysql_on_worker_shutdown(sig=None, how=None, exitcode=None, **kwargs):
    """
    Worker í”„ë¡œì„¸ìŠ¤ ì¢…ë£Œ ì‹œ ì™„ì „í•œ ì •ë¦¬
    - ëª¨ë“  Connection Pool ë‹«ê¸°
    - SSH Tunnel ì¢…ë£Œ
    - Event Loop ì •ë¦¬
    """
    logger.info("ğŸ›‘ Worker shutdown signal received, cleaning up MySQL resources...")
    
    loop = None
    
    try:
        # Event loop í™•ë³´
        try:
            loop = asyncio.get_event_loop()
        except RuntimeError:
            logger.debug("No event loop, creating new one for cleanup")
            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)
        
        # Loopê°€ ë‹«í˜€ìˆìœ¼ë©´ ìƒˆë¡œ ìƒì„±
        if loop.is_closed():
            logger.debug("Event loop closed, creating new one for cleanup")
            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)
        
        # MySQL í´ë¼ì´ì–¸íŠ¸ ì •ë¦¬
        if not loop.is_running():
            # ì •ìƒ ì¼€ì´ìŠ¤: loopê°€ ë©ˆì¶°ìˆìŒ
            loop.run_until_complete(mysql_client.close())
            logger.info("âœ… MySQL connection pools and SSH tunnel closed")
        else:
            # ë¹„ì •ìƒ ì¼€ì´ìŠ¤: loopê°€ ì—¬ì „íˆ ì‹¤í–‰ ì¤‘
            logger.warning(
                "âš ï¸ Event loop still running during shutdown, "
                "attempting forced cleanup..."
            )
            try:
                future = asyncio.ensure_future(mysql_client.close(), loop=loop)
                loop.run_until_complete(asyncio.wait_for(future, timeout=5.0))
                logger.info("âœ… Forced cleanup completed")
            except asyncio.TimeoutError:
                logger.error("âŒ Timeout during forced MySQL cleanup (5s)")
            except Exception as e:
                logger.error(f"âŒ Error during forced cleanup: {e}")
    
    except Exception as e:
        logger.error(f"âŒ Error during worker shutdown cleanup: {e}", exc_info=True)
    
    finally:
        # Event loop ì •ë¦¬
        if loop and not loop.is_closed():
            try:
                # ë‚¨ì€ íƒœìŠ¤í¬ ì·¨ì†Œ
                pending = asyncio.all_tasks(loop)
                for task in pending:
                    task.cancel()
                
                # Loop ì¢…ë£Œ
                loop.close()
                logger.info("âœ… Event loop closed")
            except Exception as e:
                logger.error(f"Error closing event loop: {e}")


# =============================================================================
# Pool ìƒíƒœ ì¡°íšŒ ìœ í‹¸ë¦¬í‹°
# =============================================================================

def get_mysql_pool_stats() -> dict:
    """
    MySQL Connection Pool í†µê³„ ë°˜í™˜
    
    Returns:
        dict: {
            "pools": {db_name: {size, freesize, maxsize, usage_percent}},
            "ssh_tunnel": {active, local_port, initialized}
        }
    """
    try:
        stats = {"pools": {}}
        
        # ê° DB Pool í†µê³„
        for db_name, pool in mysql_client.pools.items():
            # aiomysql Poolì˜ ì‹¤ì œ ì†ì„± í™•ì¸
            # size()ì™€ freesize()ëŠ” ë©”ì„œë“œì…ë‹ˆë‹¤
            try:
                size = pool.size  # í˜„ì¬ ìƒì„±ëœ ì—°ê²° ìˆ˜
                freesize = pool.freesize  # ì‚¬ìš© ê°€ëŠ¥í•œ ì—°ê²° ìˆ˜
                maxsize = pool.maxsize  # ìµœëŒ€ ì—°ê²° ìˆ˜ (ì†ì„±)
                minsize = pool.minsize  # ìµœì†Œ ì—°ê²° ìˆ˜ (ì†ì„±)
                
                in_use = size - freesize
                usage_percent = round((in_use / maxsize) * 100, 2) if maxsize > 0 else 0
                
                stats["pools"][db_name] = {
                    "size": size,
                    "freesize": freesize,
                    "in_use": in_use,
                    "minsize": minsize,
                    "maxsize": maxsize,
                    "usage_percent": usage_percent
                }
            except AttributeError as ae:
                logger.warning(f"Pool {db_name} attribute error: {ae}")
                # Pool ê°ì²´ì˜ ì‹¤ì œ ì†ì„± ë¡œê¹… (ë””ë²„ê¹…ìš©)
                logger.debug(f"Available pool attributes: {dir(pool)}")
                stats["pools"][db_name] = {"error": "Unable to read pool stats"}
        
        # SSH í„°ë„ ìƒíƒœ
        stats["ssh_tunnel"] = {
            "active": mysql_client.ssh_tunnel.is_active if mysql_client.ssh_tunnel else False,
            "local_port": mysql_client.local_port,
            "initialized": mysql_client._initialized
        }
        
        return stats
        
    except Exception as e:
        logger.error(f"Error getting pool stats: {e}", exc_info=True)
        return {"error": str(e)}
    
    
#########################
# ì‹¤ì œ íë¦„ ì˜ˆì‹œ
#########################
# 1. Worker ì‹œì‘
# $ celery -A backend.app.celery_app.celery_config worker

# â†’ worker_process_init ì‹¤í–‰
#   âœ… SSH Tunnel ìƒì„±
#   âœ… Event Loop ì„¤ì •
#   (ë¡œê·¸) "Worker started: MySQL client initialized"

# # 2. Task 1 ì‹¤í–‰: update_not_ordered_cohort
# â†’ task_prerun ì‹¤í–‰
#   âœ… Health check ìˆ˜í–‰
#   âœ… ì—°ê²° ì •ìƒ
#   (ë¡œê·¸) "MySQL healthy, starting task"

# â†’ Task ì‹¤í–‰
#   ğŸ“¥ MySQLì—ì„œ ë°ì´í„° ì¶”ì¶œ
#   ğŸ”„ ë°ì´í„° ë³€í™˜
#   ğŸ“¤ Google Sheets ì—…ë°ì´íŠ¸

# â†’ task_postrun ì‹¤í–‰
#   âœ… Pool ì‚¬ìš©ë¥  ì²´í¬: 40%
#   (ë¡œê·¸) "Pool usage: 2/5 connections"

# # 3. Task 2 ì‹¤í–‰: update_active_accounts_cohort
# â†’ task_prerun ì‹¤í–‰
#   âš ï¸ Health check ì‹¤íŒ¨
#   ğŸ”„ ìë™ ì¬ì—°ê²° ì‹œë„
#   âœ… ì¬ì—°ê²° ì„±ê³µ
#   (ë¡œê·¸) "MySQL reconnected successfully"

# â†’ Task ì‹¤í–‰
#   (ì„±ê³µ)

# â†’ task_postrun ì‹¤í–‰
#   (ì •ìƒ)

# # 4. Task 3 ì‹¤í–‰: update_incoming_leads_cohort
# â†’ task_prerun ì‹¤í–‰
#   (ì •ìƒ)

# â†’ Task ì‹¤í–‰
#   âŒ ì˜ˆì™¸ ë°œìƒ: ValueError

# â†’ task_failure ì‹¤í–‰
#   (ë¡œê·¸) "Task failed (non-connection error)"
  
# # 5. Worker ì¢…ë£Œ
# $ Ctrl+C

# â†’ worker_process_shutdown ì‹¤í–‰
#   âœ… ëª¨ë“  Connection Pool ë‹«ê¸°
#   âœ… SSH Tunnel ì¢…ë£Œ
#   âœ… Event Loop ì •ë¦¬
#   (ë¡œê·¸) "MySQL connection pools and SSH tunnel closed"