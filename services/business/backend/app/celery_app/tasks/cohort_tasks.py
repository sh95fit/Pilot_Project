"""
ì½”í˜¸íŠ¸ Task ì •ì˜
- DatabaseTask ê¸°ë°˜ìœ¼ë¡œ MySQL/Sheets ì‘ì—… ìˆ˜í–‰
- ê³µí†µ íŒŒì´í”„ë¼ì¸ ì¬ì‚¬ìš©
"""
import logging
import asyncio
from datetime import datetime

from backend.app.celery_app.celery_config import celery_app, get_mysql_pool_stats
from backend.app.celery_app.config import CohortTaskConfig
from backend.app.celery_app.tasks.base import DatabaseTask
from backend.app.celery_app.tasks.utils.data_processor import (
    DataProcessor,
    get_next_business_date
)
from backend.app.celery_app.tasks.utils.sheets_updater import SheetsUpdater
from backend.app.core.database.mysql_client import mysql_client


logger = logging.getLogger(__name__)


# =============================================================================
# ê³µí†µ íŒŒì´í”„ë¼ì¸
# =============================================================================

def run_cohort_pipeline(
    task_instance: DatabaseTask,
    config: dict,
    target: str = None,
    start_date: str = None,
    end_date: str = None
) -> dict:
    """
    ê³µí†µ íŒŒì´í”„ë¼ì¸: Extract â†’ Transform â†’ Load
    
    Args:
        task_instance: DatabaseTask ì¸ìŠ¤í„´ìŠ¤ (self)
        config: Task ì„¤ì • ë”•ì…”ë„ˆë¦¬
        target: íƒ€ê²Ÿ ë‚ ì§œ or ì œëª© (ì˜µì…˜)
        start_date: ì‹œì‘ ë‚ ì§œ (ì˜µì…˜)
        end_date: ì¢…ë£Œ ë‚ ì§œ (ì˜µì…˜)
        
    Returns:
        ì‹¤í–‰ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
    """
    try:
        # 1. Extract: MySQL ë°ì´í„° ì¶”ì¶œ
        # íŒŒë¼ë¯¸í„° ê²°ì •
        if config.get("needs_period", False):
            # ê¸°ê°„ ì¡°íšŒ ëª¨ë“œ
            if not start_date or not end_date:
                raise ValueError("needs_period=True requires both start_date and end_date")
            params = (start_date, end_date)
            logger.info(f"ğŸ“… Period mode: {start_date} ~ {end_date}")
            
        elif config.get("needs_target_date", False):
            # ë‹¨ì¼ ë‚ ì§œ ì¡°íšŒ ëª¨ë“œ
            if not target:
                raise ValueError("needs_target_date=True requires target parameter")
            params = (target,)
            logger.info(f"ğŸ“… Single date mode: {target}")
                        
        else:
            # íŒŒë¼ë¯¸í„° ì—†ìŒ
            params = ()
            logger.info(f"ğŸ“… No parameter mode")
        
        # MySQL í”„ë¡œì‹œì € ì‹¤í–‰
        raw_data = task_instance.run_async(
            task_instance.mysql.execute_procedure(
                config["procedure_name"], 
                params
            )
        )
        
        if not raw_data:
            logger.warning(f"âš ï¸ No data returned: {config['worksheet_name']}")
            return {"status": "no_data", "count": 0}
        
        logger.info(f"ğŸ“¥ Extracted {len(raw_data)} records from MySQL")
        
        # 2. Transform: ë°ì´í„° ë³€í™˜
        sheet_data = DataProcessor.to_sheets_format(raw_data)
        
        # 3. Load: Google Sheets ì—…ë°ì´íŠ¸
        updater = SheetsUpdater(task_instance.sheets, task_instance.run_async)
        
        # ê¸°ì¡´ ë°ì´í„° ì´ˆê¸°í™”
        updater.clear_data_range(
            config["spreadsheet_id"],
            config["worksheet_name"]
        )
        
        # í—¤ë” ì—…ë°ì´íŠ¸ (í•„ìš”ì‹œ)
        if config.get("needs_date_header", False):
            header_range = config.get("header_range", "A2")
            merge_cells = config.get("header_merge_cells", 1)
            
            # í—¤ë” í…ìŠ¤íŠ¸ ê²°ì •
            if config.get("needs_period", False):
                # ê¸°ê°„ ëª¨ë“œ: "2024-01-01 ~ 2024-01-31" í˜•ì‹
                header_text = f"{start_date} ~ {end_date}"
            elif config.get("needs_target_date", False):
                # ë‹¨ì¼ ë‚ ì§œ ëª¨ë“œ
                header_text = target
            else:
                # ê¸°íƒ€: í˜„ì¬ ë‚ ì§œ
                header_text = datetime.now().strftime("%Y-%m-%d")
            
            updater.update_header(
                config["spreadsheet_id"],
                config["worksheet_name"],
                header_text,
                header_range=header_range,
                merge_cells=merge_cells
            )
        
        # ë°ì´í„° ì‚½ì…
        start_cell = config["start_cell"]
        logger.info(f"ğŸ“¤ Inserting {len(sheet_data)} rows to Sheets")
        
        updater.insert_data(
            config["spreadsheet_id"],
            config["worksheet_name"],
            sheet_data,
            start_cell
        )
        
        logger.info(f"âœ… {config['worksheet_name']}: {len(raw_data)} rows updated")
        
        return {
            "status": "success",
            "count": len(raw_data),
            "target": target,
            "start_date": start_date,
            "end_date": end_date,
            "worksheet": config["worksheet_name"],
            "updated_at": datetime.now().isoformat()
        }
        
    except Exception as e:
        logger.error(f"âŒ Pipeline failed for {config.get('worksheet_name', 'unknown')}: {e}", exc_info=True)
        raise


# =============================================================================
# Cohort Task ì •ì˜
# =============================================================================

@celery_app.task(
    bind=True,
    base=DatabaseTask,
    name="cohort_tasks.update_not_ordered_cohort",
    max_retries=3,
    default_retry_delay=300
)
def update_not_ordered_cohort(self):
    """ë¯¸ì£¼ë¬¸ ê³ ê°ì‚¬ ì—…ë°ì´íŠ¸"""
    try:
        target_date = get_next_business_date()
        return run_cohort_pipeline(self, CohortTaskConfig.NOT_ORDERED, target_date)
    except Exception as e:
        raise self.retry(exc=e)


@celery_app.task(
    bind=True,
    base=DatabaseTask,
    name="cohort_tasks.update_pending_not_ordered_cohort",
    max_retries=3,
    default_retry_delay=300
)
def update_pending_not_ordered_cohort(self):
    """ì˜¤í›„ 2ì‹œ ë¯¸ì£¼ë¬¸ ê³ ê°ì‚¬ ì—…ë°ì´íŠ¸"""
    try:
        target_date = get_next_business_date()
        return run_cohort_pipeline(self, CohortTaskConfig.PENDING_NOT_ORDERED, target_date)
    except Exception as e:
        raise self.retry(exc=e)


@celery_app.task(
    bind=True,
    base=DatabaseTask,
    name="cohort_tasks.update_end_of_use_cohort",
    max_retries=3,
    default_retry_delay=300
)    
def update_end_of_use_cohort(self):
    """ì„œë¹„ìŠ¤ ì´ìš© ì¢…ë£Œ(ì´íƒˆ) ê³ ê°ì‚¬ ì—…ë°ì´íŠ¸"""
    try:
        return run_cohort_pipeline(self, CohortTaskConfig.END_OF_USE)
    except Exception as e:
        raise self.retry(exc=e)


@celery_app.task(
    bind=True,
    base=DatabaseTask,
    name="cohort_tasks.update_active_accounts_cohort",
    max_retries=3,
    default_retry_delay=300
)    
def update_active_accounts_cohort(self):
    """í™œì„± ê³ ê° ë°ì´í„° ì—…ë°ì´íŠ¸"""
    try:
        return run_cohort_pipeline(self, CohortTaskConfig.ACTIVE_ACCOUNTS)
    except Exception as e:
        raise self.retry(exc=e)


@celery_app.task(
    bind=True,
    base=DatabaseTask,
    name="cohort_tasks.update_incoming_leads_cohort",
    max_retries=3,
    default_retry_delay=300
)    
def update_incoming_leads_cohort(self):
    """ì–´ë“œë¯¼ ìœ ì… ë¦¬ë“œ ì—…ë°ì´íŠ¸"""
    try:
        target = "ì–´ë“œë¯¼ ìœ ì… ìˆ˜"
        return run_cohort_pipeline(self, CohortTaskConfig.INCOMING_LEADS, target)
    except Exception as e:
        raise self.retry(exc=e)


@celery_app.task(
    bind=True,
    base=DatabaseTask,
    name="cohort_tasks.update_now_active_accounts_cohort",
    max_retries=3,
    default_retry_delay=300
)    
def update_now_active_accounts_cohort(self, start_date=None, end_date=None):
    """í˜„ì¬ í™œì„± ê³ ê° ë°ì´í„° ì—…ë°ì´íŠ¸"""
    try:
        if start_date is None:
            start_date = "2022-12-01"
        if end_date is None:
            end_date = get_next_business_date()
        
        return run_cohort_pipeline(
            self,
            CohortTaskConfig.NOW_ACTIVE_ACCOUNTS,
            start_date=start_date,
            end_date=end_date,
        )
    except Exception as e:
        raise self.retry(exc=e)

@celery_app.task(
    bind=True,
    base=DatabaseTask,
    name="cohort_tasks.update_product_sales_summary_cohort",
    max_retries=3,
    default_retry_delay=300
)    
def update_product_sales_summary_cohort(self):
    """ì¼ë³„ ìƒí’ˆë³„ ì£¼ë¬¸ ìˆ˜ëŸ‰ ë°ì´í„° ì—…ë°ì´íŠ¸"""
    try:
        return run_cohort_pipeline(self, CohortTaskConfig.PRODUCT_SALES_SUMMARY)
    except Exception as e:
        raise self.retry(exc=e)

@celery_app.task(
    bind=True,
    base=DatabaseTask,
    name="cohort_tasks.update_account_orders_summary_cohort",
    max_retries=3,
    default_retry_delay=300
)    
def update_account_orders_summary_cohort(self):
    """ê³ ê°ì‚¬ë³„ ì£¼ë¬¸ í˜„í™© ë°ì´í„° ì—…ë°ì´íŠ¸"""
    try:
        return run_cohort_pipeline(self, CohortTaskConfig.ACCOUNT_ORDERS_SUMMARY)
    except Exception as e:
        raise self.retry(exc=e)
    
    
# ============================================
# ìƒˆ Task ì¶”ê°€ ê°€ì´ë“œ
# ============================================
"""
1. config.pyì— ì„¤ì • ì¶”ê°€:
   class CohortTaskConfig:
       NEW_TASK = {
           "spreadsheet_id": ...,
           "worksheet_name": ...,
           "procedure_name": ...,
           "needs_target_date": True/False,
           "needs_date_header": True/False,
       }

2. cohort_tasks.pyì— Task í•¨ìˆ˜ ì¶”ê°€:
   @celery_app.task(bind=True, base=DatabaseTask, name="cohort_tasks.new_task")
   def update_new_cohort(self):
       try:
           target_date = get_next_business_date()  # í•„ìš”ì‹œ
           return run_cohort_pipeline(self, CohortTaskConfig.NEW_TASK, target_date)
       except Exception as e:
           raise self.retry(exc=e)

3. celery_config.pyì— ìŠ¤ì¼€ì¤„ ì¶”ê°€:
   beat_schedule = {
       "update-new-cohort": {
           "task": "cohort_tasks.update_new_cohort",
           "schedule": crontab(...),
           "options": {"queue": "cohort"}
       }
   }

ë! íŒŒì´í”„ë¼ì¸ ë¡œì§ì€ ì¬ì‚¬ìš©
"""



# =============================================================================
# MySQL ì—°ê²° ëª¨ë‹ˆí„°ë§ Task
# =============================================================================

@celery_app.task(
    bind=True,
    name="cohort_tasks.monitor_mysql_health",
    max_retries=0
)
def monitor_mysql_health(self):
    """
    MySQL ì—°ê²° ìƒíƒœ ë° Pool ì‚¬ìš©ë¥  ëª¨ë‹ˆí„°ë§
    - SSH Tunnel ìƒíƒœ í™•ì¸
    - Connection Pool ì‚¬ìš©ë¥  í™•ì¸
    - 80% ì´ìƒ ì‚¬ìš© ì‹œ ê²½ê³ 
    """
    
    async def _async_monitor():
        """ë¹„ë™ê¸° ëª¨ë‹ˆí„°ë§ ë¡œì§"""
        try:
            # 1. Pool í†µê³„ ìˆ˜ì§‘
            stats = get_mysql_pool_stats()
            
            # 2. SSH í„°ë„ ìƒíƒœ í™•ì¸
            ssh_status = stats.get("ssh_tunnel", {})
            if not ssh_status.get("active"):
                logger.error("âŒ SSH Tunnel is NOT active!")
                return {"status": "error", "message": "SSH tunnel inactive"}
            
            logger.info(f"âœ… SSH Tunnel active on port {ssh_status.get('local_port')}")
            
            # 3. ê° Pool ìƒíƒœ í™•ì¸
            pool_warnings = []
            pools_data = stats.get("pools", {})
            
            for db_name, pool_stats in pools_data.items():
                usage = pool_stats.get("usage_percent", 0)
                logger.info(
                    f"ğŸ“Š Pool [{db_name}] - "
                    f"Size: {pool_stats['size']}/{pool_stats['maxsize']}, "
                    f"Free: {pool_stats['freesize']}, "
                    f"In-use: {pool_stats['in_use']}, "
                    f"Usage: {usage}%"
                )
                
                # ì‚¬ìš©ë¥  80% ì´ìƒ ì‹œ ê²½ê³ 
                if usage >= 80:
                    warning_msg = (
                        f"High connection usage for {db_name}: {usage}%! "
                        f"Consider increasing maxsize or checking for connection leaks."
                    )
                    logger.warning(f"âš ï¸ {warning_msg}")
                    pool_warnings.append(warning_msg)
            
            # 4. Health check ìˆ˜í–‰
            is_healthy = await mysql_client.health_check()
            
            if not is_healthy:
                logger.error("âŒ MySQL health check failed!")
                return {
                    "status": "unhealthy",
                    "ssh_tunnel": ssh_status,
                    "pools": pools_data,
                    "warnings": pool_warnings
                }
            
            logger.info("âœ… MySQL health check passed")
            return {
                "status": "healthy",
                "ssh_tunnel": ssh_status,
                "pools": pools_data,
                "warnings": pool_warnings
            }
        
        except Exception as e:
            logger.error(f"âŒ Error in MySQL monitoring: {e}", exc_info=True)
            return {"status": "error", "message": str(e)}
    
    # ë©”ì¸ ì‹¤í–‰ ë¡œì§
    try:
        # Event loop ì•ˆì „í•˜ê²Œ ê°€ì ¸ì˜¤ê¸°
        try:
            loop = asyncio.get_event_loop()
            if loop.is_closed():
                loop = asyncio.new_event_loop()
                asyncio.set_event_loop(loop)
        except RuntimeError:
            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)
        
        # ë¹„ë™ê¸° ëª¨ë‹ˆí„°ë§ ì‹¤í–‰
        result = loop.run_until_complete(_async_monitor())
        return result
    
    except Exception as e:
        logger.error(f"âŒ Failed to execute monitoring task: {e}", exc_info=True)
        return {"status": "error", "message": str(e)}